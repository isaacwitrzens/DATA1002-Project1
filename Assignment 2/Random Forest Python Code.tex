import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

base_url = "https://raw.githubusercontent.com/isaacwitrzens/DATA1002-Project1/main/Assignment%202/"

train = pd.read_csv(base_url + "train.csv")
valid = pd.read_csv(base_url + "valid.csv")
test = pd.read_csv(base_url + "test.csv")

target = "TotalCrimes"

X_train = train.drop(columns=[target])
y_train = train[target]

X_valid = valid.drop(columns=[target])
y_valid = valid[target]

X_test = test.drop(columns=[target])
y_test = test[target]

# Define parameter ranges
n_estimators_list = [100, 200]
max_depth_list = [5, None]
max_features_list = ["sqrt", None]
min_samples_split_list = [2, 5]

results = []

# Grid search for hyperparameter tuning
for n_estimators in n_estimators_list:
    for max_depth in max_depth_list:
        for max_features in max_features_list:
            for min_samples_split in min_samples_split_list:
                rf = RandomForestRegressor(
                    n_estimators=n_estimators,
                    max_depth=max_depth,
                    max_features=max_features,
                    min_samples_split=min_samples_split,
                    random_state=42,
                    n_jobs=-1
                )
                rf.fit(X_train, y_train)
                preds_valid = rf.predict(X_valid)

                r2 = r2_score(y_valid, preds_valid)
                mae = mean_absolute_error(y_valid, preds_valid)
                rmse = mean_squared_error(y_valid, preds_valid, squared=False)

                results.append({
                    "n_estimators": n_estimators,
                    "max_depth": max_depth,
                    "max_features": max_features,
                    "min_samples_split": min_samples_split,
                    "val_r2": r2,
                    "val_mae": mae,
                    "val_rmse": rmse
                })

results_df = pd.DataFrame(results)

# Find best parameters
best_row = results_df.sort_values("val_r2", ascending=False).iloc[0]
best_params = {
    "n_estimators": int(best_row["n_estimators"]),
    "max_depth": None if pd.isna(best_row["max_depth"]) else best_row["max_depth"],
    "max_features": None if best_row["max_features"] == "None" else best_row["max_features"],
    "min_samples_split": int(best_row["min_samples_split"])
}

print("\nBest parameters (validation):")
print(best_params)
print(best_row)

# Retrain on training data with best parameters
rf_best = RandomForestRegressor(
    n_estimators = best_params["n_estimators"],
    max_depth = best_params["max_depth"],
    max_features = best_params["max_features"],
    min_samples_split = best_params["min_samples_split"],
    random_state = 42,
    n_jobs = -1
)

rf_best.fit(X_train, y_train)

# Evaluate on test set
preds_test = rf_best.predict(X_test)
test_r2 = r2_score(y_test, preds_test)
test_mae = mean_absolute_error(y_test, preds_test)
test_rmse = mean_squared_error(y_test, preds_test, squared=False)

print("\nTest performance")
print("RÂ²:   " + str(round(test_r2, 4)))
print("MAE:  " + str(round(test_mae, 2)))
print("RMSE: " + str(round(test_rmse, 2)))


# Predicted vs Actual Plot
plt.figure(figsize=(6, 6))
plt.scatter(y_test, preds_test, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color="red", linestyle="--")
plt.xlabel("Actual Total Crimes")
plt.ylabel("Predicted Total Crimes")
plt.title("Predicted vs Actual Values (Test Set)")
plt.tight_layout()
plt.show()


# Residuals Distribution Plot
residuals = y_test - preds_test

plt.figure(figsize=(8, 5))
plt.hist(residuals, bins=30, edgecolor="black", alpha=0.7)
plt.axvline(x=0, color="red", linestyle="--")
plt.xlabel("Residual (Actual - Predicted)")
plt.ylabel("Frequency")
plt.title("Distribution of Residuals (Test Set)")
plt.tight_layout()
plt.show()


# Feature Importances Plot
importances = rf_best.feature_importances_
feature_names = X_train.columns
feature_importances = pd.DataFrame({
    "Feature": feature_names,
    "Importance": importances * 100
})
feature_importances = feature_importances.sort_values("Importance", ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importances["Feature"], feature_importances["Importance"])
plt.xlabel("Importance (%)")
plt.ylabel("Feature")
plt.title("Feature Importances in Random Forest Model")
plt.gca().invert_yaxis()  # most important on top
plt.tight_layout()
plt.show()
